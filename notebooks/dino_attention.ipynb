{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ViT DINO Attention Maps**\n",
    "\n",
    "**DINO ViT Model choice**\n",
    "\n",
    "You can choose between the following models:\n",
    "- `vit_small_patch14_518.dinov2`\n",
    "- `vit_base_patch14_518.dinov2`\n",
    "- `vit_large_patch14_518.dinov2`\n",
    "- `vit_small_patch16_224.dino`\n",
    "- `vit_small_patch8_224.dino`\n",
    "- `vit_base_patch16_224.dino`\n",
    "- `vit_base_patch8_224.dino`\n",
    "\n",
    "Remember to change the image size in the dataset loader accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dev extension to use pytorch\n",
    "! pip install \"mlx-image[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxim.model import create_model\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlxim.transform import ImageNetTransform\n",
    "from mlxim.data import FolderDataset, DataLoader\n",
    "from mlxim.io import read_rgb\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from mlxim.io import read_rgb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FolderDataset(\n",
    "    root_dir=\"../data/test/cat\",\n",
    "    transform=ImageNetTransform(\n",
    "        train=False,\n",
    "        img_size=224,\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vit_small_patch8_224.dino\"\n",
    "\n",
    "model = create_model(model_name=model_name)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random image\n",
    "idx = random.choice(range(len(dataset)))\n",
    "image_path = dataset.images[idx]\n",
    "x = mx.expand_dims(dataset[idx], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the attention masks from all heads\n",
    "_, attn_mask = model(x, attn_masks=True)\n",
    "attn = attn_mask[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some parameters\n",
    "_, w, h, _ = x.shape\n",
    "\n",
    "patch_size = model.patch_size\n",
    "\n",
    "w_featmap = w // patch_size\n",
    "h_featmap = h // patch_size\n",
    "\n",
    "w = w - w % patch_size\n",
    "h = h - h % patch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot single attention maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply some threshold to get only most of the attention masses\n",
    "# threshold = 0.75\n",
    "threshold = None\n",
    "\n",
    "# switching to torch because it's easier to work with\n",
    "attentions = torch.tensor(np.array(attn))\n",
    "\n",
    "nh = attentions.shape[1]  # number of head\n",
    "\n",
    "# we keep only the output patch attention\n",
    "attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "th_attn = None\n",
    "if threshold is not None:\n",
    "    # we keep only a certain percentage of the mass\n",
    "    val, idx = torch.sort(attentions)\n",
    "    val /= torch.sum(val, dim=1, keepdim=True)\n",
    "    cumval = torch.cumsum(val, dim=1)\n",
    "    th_attn = cumval > (1 - threshold)\n",
    "    idx2 = torch.argsort(idx)\n",
    "    for head in range(nh):\n",
    "        th_attn[head] = th_attn[head][idx2[head]]\n",
    "    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "    # interpolate\n",
    "    th_attn = F.interpolate(th_attn.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].detach().cpu().numpy()\n",
    "\n",
    "attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "attentions = F.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if th_attn is not None:\n",
    "    attentions = th_attn\n",
    "\n",
    "img = read_rgb(image_path)\n",
    "img = cv2.resize(img, (w, h))\n",
    "for i, v in enumerate(attentions):\n",
    "    # Attention from the output token to the input space.\n",
    "    # mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "    mask = v[..., np.newaxis]\n",
    "    # result = (mask / mask.max() * img).astype(\"uint8\")\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax2.set_title(\"Attention Map_%d Layer\" % (i + 1))\n",
    "    _ = ax1.imshow(img)\n",
    "    _ = ax2.imshow(mask / mask.max())\n",
    "\n",
    "# Summing up all the attention maps\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"All attention maps\")\n",
    "_ = ax1.imshow(img)\n",
    "mask = np.sum(attentions, axis=0)\n",
    "_ = ax2.imshow(mask / mask.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
